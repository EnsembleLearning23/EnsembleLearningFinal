{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1fcbcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52665384",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eba18f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node_cls:\n",
    "    \"\"\"\n",
    "    Initializes \"Node class constructor\" for classification problems which stores and initialises:\n",
    "    Feature index used for best split :  feature = None,\n",
    "    Left child node :  left_spl = None,\n",
    "    Right child node :  right_spl = None,\n",
    "    Node Splitting threshold value :  spl_val = None,\n",
    "    Information Value gain:  info_gain = None,\n",
    "    Output value of the node :  out_val = None\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        feature=None,\n",
    "        spl_val=None,\n",
    "        left_spl=None,\n",
    "        right_spl=None,\n",
    "        info_gain=None,\n",
    "        *,\n",
    "        out_val=None\n",
    "    ):\n",
    "        self.feature = feature\n",
    "        self.threshold = spl_val\n",
    "        self.left = left_spl\n",
    "        self.right = right_spl\n",
    "        self.info_gain = info_gain\n",
    "        self.output = out_val\n",
    "\n",
    "    # function to check whether a leaf node or not\n",
    "    def is_leaf_node(self):\n",
    "        return self.output is not None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "94cee75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeClassifier:\n",
    "    \"\"\"\n",
    "    Initializes \"Decision Tree constructor\" for classification problems which stores and initialises:\n",
    "    Maximum tree depth : max_depth=100,\n",
    "    Minimum samples reuqired for splitting nodes : min_samples_split=2,\n",
    "    Number of features to be used for tree: n_features=None,\n",
    "    Cost function/ Loss criterion for splitting nodes : criterion='entropy'\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, max_depth=100, min_samples_split=2, n_features=None, criterion=\"entropy\"\n",
    "    ):\n",
    "\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.n_features = n_features\n",
    "        self.criterion = criterion\n",
    "        self.root = None\n",
    "\n",
    "    # # For storing tree structure to generate tree graph visualization\n",
    "    #     self.tree = None\n",
    "    #     self.graph = None\n",
    "    #     self.list_node_names = []\n",
    "\n",
    "    \"\"\"\n",
    "  Defines the train method of the DecisionTree class, \n",
    "  which takes two inputs: \n",
    "  1. X, a matrix of input features\n",
    "  2. y, a vector of output labels\n",
    "  Then, uses the _build_tree function to proceed with growing the decision tree structure\n",
    "  \"\"\"\n",
    "\n",
    "    def train(self, X, y):\n",
    "        # sets self.n_features to number of features in the input if n_features is not specified,\n",
    "        # Else, the minimum of n_features and the number of features in the input data if n_features is specified to avoid exceeding the maximum feature limit\n",
    "\n",
    "        self.n_features = (\n",
    "            X.shape[1] if not self.n_features else min(self.n_features, X.shape[1])\n",
    "        )\n",
    "        self.root = self._build_tree(X, y)\n",
    "\n",
    "    \"\"\"\n",
    "  Defines the recursive \"_build_tree\" function of the DecisionTree class, \n",
    "  which takes two inputs: \n",
    "  1. X, a matrix of input features\n",
    "  2. y, a vector of output labels\n",
    "\n",
    "  Returns Tree Nodes after each iteration\n",
    "  \"\"\"\n",
    "\n",
    "    def _build_tree(self, X, y, depth=0):\n",
    "        # stores the number of observations and features in the input data\n",
    "        n_obs, n_fts = X.shape\n",
    "\n",
    "        # stores the number of output classes in the input data\n",
    "        n_classes = len(np.unique(y))\n",
    "\n",
    "        # checks for stopping conditions of a decision tree, if met returns the leaf node\n",
    "        if n_classes == 1 or depth >= self.max_depth or n_obs <= self.min_samples_split:\n",
    "            leaf_val = self._dominant_class(y)\n",
    "            return Node_cls(out_val=leaf_val)\n",
    "\n",
    "        # checks for all possible splits across features and returns the best split threshold and feature column index\n",
    "        best_gain = -1\n",
    "        best_feature, best_thresh = None, None\n",
    "        criterion = self.criterion\n",
    "\n",
    "        # random selection of specified number of features out of all the available features\n",
    "        select_fts = np.random.choice(n_fts, self.n_features, replace=False)\n",
    "\n",
    "        for feature in select_fts:\n",
    "            thresholds = np.unique(X[:, feature])\n",
    "            for thresh in thresholds:\n",
    "                gain = self._information_gain(y, X[:, feature], thresh, criterion)\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    best_feature = feature\n",
    "                    best_thresh = thresh\n",
    "\n",
    "        # create child nodes\n",
    "        left_idxs, right_idxs = self._split(X[:, best_feature], best_thresh)\n",
    "        left = self._build_tree(X[left_idxs, :], y[left_idxs], depth + 1)\n",
    "        right = self._build_tree(X[right_idxs, :], y[right_idxs], depth + 1)\n",
    "\n",
    "        return Node_cls(\n",
    "            feature=best_feature,\n",
    "            spl_val=best_thresh,\n",
    "            left_spl=left,\n",
    "            right_spl=right,\n",
    "            info_gain=best_gain,\n",
    "        )\n",
    "\n",
    "    \"\"\"\n",
    "  Defines the most commom class/ label function  \n",
    "  which takes input as y, a vector of output labels and returns the class with highest frequency\n",
    "  \"\"\"\n",
    "\n",
    "    def _dominant_class(self, y):\n",
    "        classes, class_counts = np.unique(y, return_counts=True)\n",
    "        return classes[class_counts.argmax()]\n",
    "\n",
    "    \"\"\"\n",
    "  Defines the node splitting function which takes as input\n",
    "  data : the subset input data based on a feature index\n",
    "  split_thresh : Specified split threshold to split the tree in left and right child nodes\n",
    "\n",
    "  Returns : Indices of left and right child nodes\n",
    "  \"\"\"\n",
    "\n",
    "    def _split(self, data, split_thresh):\n",
    "        left_idxs = np.argwhere(data <= split_thresh).flatten()\n",
    "        right_idxs = np.argwhere(data > split_thresh).flatten()\n",
    "        return left_idxs, right_idxs\n",
    "\n",
    "    \"\"\"\n",
    "  Defines the information_gain funtion which takes as input\n",
    "  y_parent : Parent node data for labels\n",
    "  X_ft : the subset input data based on a feature index\n",
    "  split_thresh : Specified split threshold to split the tree in left and right child nodes\n",
    "  criterion : Criterion to calculate node impurity values\n",
    "\n",
    "  Returns : Information gain value as we move from a parent node to child nodes\n",
    "  \"\"\"\n",
    "\n",
    "    def _information_gain(self, y_parent, X_ft, threshold, criterion):\n",
    "        if criterion.lower() == \"entropy\":\n",
    "            # parent entropy\n",
    "            parent_entropy = self._entropy(y_parent)\n",
    "\n",
    "            # create children\n",
    "            left_idxs, right_idxs = self._split(X_ft, threshold)\n",
    "            if len(left_idxs) == 0 or len(right_idxs) == 0:\n",
    "                return 0\n",
    "\n",
    "            # calculate the weighted avg. entropy of children\n",
    "            n = len(y_parent)\n",
    "            p_left, p_right = len(left_idxs) / n, len(right_idxs) / n\n",
    "            e_left, e_right = self._entropy(y_parent[left_idxs]), self._entropy(\n",
    "                y_parent[right_idxs]\n",
    "            )\n",
    "\n",
    "            child_entropy = p_left * e_left + p_right * e_right\n",
    "\n",
    "            # calculate the IG\n",
    "            information_gain = parent_entropy - child_entropy\n",
    "\n",
    "        elif criterion.lower() == \"gini\":\n",
    "            # parent gini impurity\n",
    "            parent_gini = self._gini_impurity(y_parent)\n",
    "\n",
    "            # create children\n",
    "            left_idxs, right_idxs = self._split(X_ft, threshold)\n",
    "            if len(left_idxs) == 0 or len(right_idxs) == 0:\n",
    "                return 0\n",
    "\n",
    "            # calculate the weighted avg. gini impurity of children\n",
    "            n = len(y_parent)\n",
    "            p_left, p_right = len(left_idxs) / n, len(right_idxs) / n\n",
    "            g_left, g_right = self._gini_impurity(\n",
    "                y_parent[left_idxs]\n",
    "            ), self._gini_impurity(y_parent[right_idxs])\n",
    "\n",
    "            child_gini = p_left * g_left + p_right * g_right\n",
    "\n",
    "            # calculate the IG\n",
    "            information_gain = parent_gini - child_gini\n",
    "\n",
    "        else:\n",
    "            information_gain = -1\n",
    "\n",
    "        return information_gain\n",
    "\n",
    "    \"\"\"\n",
    "  Defines the entropy function which takes as input\n",
    "  y_parent : data of label for a node\n",
    "\n",
    "  Returns : Entropy of the node\n",
    "  \"\"\"\n",
    "\n",
    "    def _entropy(self, y):\n",
    "        n = len(y)\n",
    "        _, counts = np.unique(y, return_counts=True)\n",
    "        ps = counts / n\n",
    "        ps_nonzero = ps[ps > 0]  # exclude labels with probability 0\n",
    "        return -np.sum(ps_nonzero * np.log2(ps_nonzero))\n",
    "\n",
    "    \"\"\"\n",
    "  Defines the gini impurity function which takes as input\n",
    "  y_parent : data of label for a node\n",
    "\n",
    "  Returns : Gini Impurity of the node\n",
    "  \"\"\"\n",
    "\n",
    "    def _gini_impurity(self, y):\n",
    "        n = len(y)\n",
    "        _, counts = np.unique(y, return_counts=True)\n",
    "        ps = counts / n\n",
    "        return 1 - np.sum(np.square(ps))\n",
    "\n",
    "    \"\"\"\n",
    "  Defines the predict method of the Decision Tree Classifier which takes as input\n",
    "  X : Test/ Valdiation dataset for features\n",
    "\n",
    "  Returns : Array of Prediction classes after traversing through each observation or row in \"X\"\n",
    "  \"\"\"\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._traverse_dtree(x, self.root) for x in X])\n",
    "\n",
    "    \"\"\"\n",
    "  Defines the recursive \"_traverse_dtree\" function which takes as input\n",
    "  x : one single row/ observation of the test/validation data\n",
    "  node : information of parent node to traverse down the tree to leaf\n",
    "  \"\"\"\n",
    "\n",
    "    def _traverse_dtree(self, x, node):\n",
    "\n",
    "        # Checks for leaf node, if satisfied returns the node output value, else keeps on traversing\n",
    "        if node.is_leaf_node():\n",
    "            return node.output\n",
    "\n",
    "        # Checks if observation has the feature with value according to left child node (<= threshold) and keeps on traversing in the left splits\n",
    "        if x[node.feature] <= node.threshold:\n",
    "            return self._traverse_dtree(x, node.left)\n",
    "\n",
    "        # Finally traverses on the right child node\n",
    "        return self._traverse_dtree(x, node.right)\n",
    "\n",
    "    \"\"\"\n",
    "    Defines the recursive \"print_tree\" function which returns the text print of the trained decision tree\n",
    "    \"\"\"\n",
    "\n",
    "    def print_tree(self, tree=None, indent=\" \"):\n",
    "        \"\"\"function to print the tree\"\"\"\n",
    "        if not tree:\n",
    "            tree = self.root\n",
    "\n",
    "        if tree.output is not None:\n",
    "            print(tree.output)\n",
    "\n",
    "        else:\n",
    "            print(\n",
    "                \"X_\" + str(tree.feature),\n",
    "                \"<=\",\n",
    "                tree.threshold,\n",
    "                \"?\",\n",
    "                round(tree.info_gain, 4),\n",
    "            )\n",
    "            print(\"%sleft:\" % (indent), end=\"\")\n",
    "            self.print_tree(tree.left, indent + indent)\n",
    "            print(\"%sright:\" % (indent), end=\"\")\n",
    "            self.print_tree(tree.right, indent + indent)\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
