{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1fcbcfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydot in c:\\users\\computop\\anaconda3\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: pyparsing>=2.1.4 in c:\\users\\computop\\anaconda3\\lib\\site-packages (from pydot) (3.0.4)\n",
      "Requirement already satisfied: graphviz in c:\\users\\computop\\anaconda3\\lib\\site-packages (0.20.1)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "!pip install pydot\n",
    "!pip install graphviz\n",
    "import pydot\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52665384",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eba18f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node_cls:\n",
    "    \"\"\"\n",
    "    Initializes \"Node class constructor\" for classification problems which stores and initialises:\n",
    "    Feature index used for best split :  feature = None,\n",
    "    Left child node :  left_spl = None,\n",
    "    Right child node :  right_spl = None,\n",
    "    Node Splitting threshold value :  spl_val = None,\n",
    "    Information Value gain:  info_gain = None,\n",
    "    Output value of the node :  out_val = None\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        feature=None,\n",
    "        spl_val=None,\n",
    "        left_spl=None,\n",
    "        right_spl=None,\n",
    "        info_gain=None,\n",
    "        *,\n",
    "        out_val=None\n",
    "    ):\n",
    "        self.feature = feature\n",
    "        self.threshold = spl_val\n",
    "        self.left = left_spl\n",
    "        self.right = right_spl\n",
    "        self.info_gain = info_gain\n",
    "        self.output = out_val\n",
    "\n",
    "    # function to check whether a leaf node or not\n",
    "    def is_leaf_node(self):\n",
    "        return self.output is not None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "94cee75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeClassifier():\n",
    "  '''\n",
    "  Initializes \"Decision Tree constructor\" for classification problems which stores and initialises:\n",
    "  Maximum tree depth : max_depth=100, \n",
    "  Minimum samples reuqired for splitting nodes : min_samples_split=2, \n",
    "  Number of features to be used for tree: n_features=None, \n",
    "  Cost function/ Loss criterion for splitting nodes : criterion='entropy'\n",
    "\n",
    "  '''\n",
    "\n",
    "  def __init__(self, max_depth=100, min_samples_split=2, n_features=None, criterion='entropy'):\n",
    "\n",
    "    self.max_depth = max_depth\n",
    "    self.min_samples_split = min_samples_split\n",
    "    self.n_features = n_features\n",
    "    self.criterion = criterion\n",
    "    self.root = None\n",
    "\n",
    "# For storing tree structure to generate tree graph visualization\n",
    "    self.tree = None\n",
    "  '''\n",
    "  Defines the train method of the DecisionTree class, \n",
    "  which takes two inputs: \n",
    "  1. X, a matrix of input features\n",
    "  2. y, a vector of output labels\n",
    "  Then, uses the _build_tree function to proceed with growing the decision tree structure\n",
    "  '''\n",
    "\n",
    "  def train(self, X, y):\n",
    "# sets self.n_features to number of features in the input if n_features is not specified, \n",
    "# Else, the minimum of n_features and the number of features in the input data if n_features is specified to avoid exceeding the maximum feature limit\n",
    "\n",
    "    self.n_features = X.shape[1] if not self.n_features else min(self.n_features, X.shape[1])\n",
    "    self.root = self._build_tree(X, y)\n",
    "    self.tree = self._tree_to_dict()\n",
    "\n",
    "\n",
    "  '''\n",
    "  Defines the recursive \"_build_tree\" function of the DecisionTree class, \n",
    "  which takes two inputs: \n",
    "  1. X, a matrix of input features\n",
    "  2. y, a vector of output labels\n",
    "\n",
    "  Returns Tree Nodes after each iteration\n",
    "  '''\n",
    "\n",
    "  def _build_tree(self, X, y, depth = 0):\n",
    "# stores the number of observations and features in the input data\n",
    "    n_obs, n_fts = X.shape\n",
    "\n",
    "# stores the number of output classes in the input data\n",
    "    n_classes = len(np.unique(y))\n",
    "\n",
    "# checks for stopping conditions of a decision tree, if met returns the leaf node\n",
    "    if (n_classes==1 or depth >= self.max_depth or n_obs <= self.min_samples_split):\n",
    "      leaf_val = self._dominant_class(y)\n",
    "      return Node_cls(out_val = leaf_val)\n",
    "\n",
    "# checks for all possible splits across features and returns the best split threshold and feature column index\n",
    "    best_gain = -1\n",
    "    best_feature, best_thresh = None, None\n",
    "    criterion = self.criterion\n",
    "\n",
    "# random selection of specified number of features out of all the available features \n",
    "    select_fts = np.random.choice(n_fts, self.n_features, replace=False)\n",
    "\n",
    "    for feature in select_fts:\n",
    "      thresholds = np.unique(X[:, feature])\n",
    "      for thresh in thresholds:\n",
    "        gain = self._information_gain(y, X[:, feature], thresh, criterion)\n",
    "        if gain > best_gain:\n",
    "              best_gain = gain\n",
    "              best_feature = feature\n",
    "              best_thresh = thresh\n",
    "\n",
    "\n",
    "# create child nodes\n",
    "    left_idxs, right_idxs = self._split(X[:, best_feature], best_thresh)\n",
    "    left = self._build_tree(X[left_idxs, :], y[left_idxs], depth + 1)\n",
    "    right = self._build_tree(X[right_idxs, :], y[right_idxs], depth + 1)\n",
    "\n",
    "    return Node_cls(feature = best_feature, spl_val = best_thresh, left_spl = left, right_spl = right, info_gain = best_gain)\n",
    "    \n",
    "\n",
    "  '''\n",
    "  Defines the most commom class/ label function  \n",
    "  which takes input as y, a vector of output labels and returns the class with highest frequency\n",
    "  '''\n",
    "  def _dominant_class(self, y):\n",
    "    classes, class_counts = np.unique(y, return_counts=True)\n",
    "    return classes[class_counts.argmax()]\n",
    "\n",
    "  '''\n",
    "  Defines the node splitting function which takes as input\n",
    "  data : the subset input data based on a feature index\n",
    "  split_thresh : Specified split threshold to split the tree in left and right child nodes\n",
    "\n",
    "  Returns : Indices of left and right child nodes\n",
    "  '''\n",
    "\n",
    "  def _split(self, data, split_thresh):\n",
    "    left_idxs = np.argwhere(data <= split_thresh).flatten()\n",
    "    right_idxs = np.argwhere(data > split_thresh).flatten()\n",
    "    return left_idxs, right_idxs\n",
    "\n",
    "  '''\n",
    "  Defines the information_gain funtion which takes as input\n",
    "  y_parent : Parent node data for labels\n",
    "  X_ft : the subset input data based on a feature index\n",
    "  split_thresh : Specified split threshold to split the tree in left and right child nodes\n",
    "  criterion : Criterion to calculate node impurity values\n",
    "\n",
    "  Returns : Information gain value as we move from a parent node to child nodes\n",
    "  '''\n",
    "\n",
    "  def _information_gain(self, y_parent, X_ft, threshold, criterion):\n",
    "    if criterion.lower() == 'entropy':\n",
    "      # parent entropy\n",
    "      parent_entropy = self._entropy(y_parent)\n",
    "\n",
    "      # create children\n",
    "      left_idxs, right_idxs = self._split(X_ft, threshold)\n",
    "      if len(left_idxs) == 0 or len(right_idxs) == 0:\n",
    "          return 0\n",
    "      \n",
    "      # calculate the weighted avg. entropy of children\n",
    "      n = len(y_parent)\n",
    "      p_left, p_right = len(left_idxs)/n , len(right_idxs)/n\n",
    "      e_left, e_right = self._entropy(y_parent[left_idxs]), self._entropy(y_parent[right_idxs])\n",
    "\n",
    "      child_entropy = p_left*e_left + p_right*e_right\n",
    "\n",
    "      # calculate the IG\n",
    "      information_gain = parent_entropy - child_entropy\n",
    "\n",
    "    elif criterion.lower() == 'gini' :\n",
    "      # parent gini impurity\n",
    "      parent_gini = self._gini_impurity(y_parent)\n",
    "\n",
    "      # create children\n",
    "      left_idxs, right_idxs = self._split(X_ft, threshold)\n",
    "      if len(left_idxs) == 0 or len(right_idxs) == 0:\n",
    "          return 0\n",
    "      \n",
    "      # calculate the weighted avg. gini impurity of children\n",
    "      n = len(y_parent)\n",
    "      p_left, p_right = len(left_idxs)/n , len(right_idxs)/n\n",
    "      g_left, g_right = self._gini_impurity(y_parent[left_idxs]), self._gini_impurity(y_parent[right_idxs])\n",
    "      \n",
    "      child_gini = p_left*g_left + p_right*g_right\n",
    "\n",
    "      # calculate the IG\n",
    "      information_gain = parent_gini - child_gini\n",
    "\n",
    "    else :\n",
    "      information_gain = -1\n",
    "\n",
    "    return information_gain\n",
    "\n",
    "  '''\n",
    "  Defines the entropy function which takes as input\n",
    "  y_parent : data of label for a node\n",
    "\n",
    "  Returns : Entropy of the node\n",
    "  '''\n",
    "  def _entropy(self, y):\n",
    "    n = len(y)\n",
    "    _, counts = np.unique(y, return_counts=True)\n",
    "    ps = counts / n\n",
    "    ps_nonzero = ps[ps > 0]  # exclude labels with probability 0\n",
    "    return -np.sum(ps_nonzero * np.log2(ps_nonzero))\n",
    "\n",
    "  '''\n",
    "  Defines the gini impurity function which takes as input\n",
    "  y_parent : data of label for a node\n",
    "\n",
    "  Returns : Gini Impurity of the node\n",
    "  '''\n",
    "  def _gini_impurity(self, y):\n",
    "    n = len(y)\n",
    "    _, counts = np.unique(y, return_counts=True)\n",
    "    ps = counts / n\n",
    "    return 1 - np.sum(np.square(ps))\n",
    "\n",
    "  '''\n",
    "  Defines the predict method of the Decision Tree Classifier which takes as input\n",
    "  X : Test/ Valdiation dataset for features\n",
    "\n",
    "  Returns : Array of Prediction classes after traversing through each observation or row in \"X\"\n",
    "  '''\n",
    "\n",
    "  def predict(self, X):\n",
    "    return np.array([self._traverse_dtree(x, self.root) for x in X])\n",
    "\n",
    "  '''\n",
    "  Defines the recursive \"_traverse_dtree\" function which takes as input\n",
    "  x : one single row/ observation of the test/validation data\n",
    "  node : information of parent node to traverse down the tree to leaf\n",
    "  '''\n",
    "  def _traverse_dtree(self, x, node):\n",
    "\n",
    "# Checks for leaf node, if satisfied returns the node output value, else keeps on traversing\n",
    "    if node.is_leaf_node():\n",
    "            return node.output\n",
    "\n",
    "# Checks if observation has the feature with value according to left child node (<= threshold) and keeps on traversing in the left splits\n",
    "    if x[node.feature] <= node.threshold:\n",
    "        return self._traverse_dtree(x, node.left)\n",
    "\n",
    "# Finally traverses on the right child node\n",
    "    return self._traverse_dtree(x, node.right)\n",
    "\n",
    "  '''\n",
    "  Defines the recursive \"print_tree\" function which returns the text print of the trained decision tree\n",
    "  '''\n",
    "\n",
    "  def print_tree(self, tree=None, indent=\" \"):\n",
    "    ''' function to print the tree '''\n",
    "    if not tree:\n",
    "        tree = self.root\n",
    "\n",
    "    if tree.output is not None:\n",
    "        print(tree.output)\n",
    "\n",
    "    else:\n",
    "        print(\"X_\"+str(tree.feature), \"<=\", tree.threshold, \"?\", round(tree.info_gain,4))\n",
    "        print(\"%sleft:\" % (indent), end=\"\")\n",
    "        self.print_tree(tree.left, indent + indent)\n",
    "        print(\"%sright:\" % (indent), end=\"\")\n",
    "        self.print_tree(tree.right, indent + indent)\n",
    "\n",
    "  '''\n",
    "  Defines the recursive \"_tree_to_dict\" function which returns the dictionary of nodes of the trained decision tree\n",
    "  '''\n",
    "\n",
    "  def _tree_to_dict(self, tree=None):\n",
    "      ''' function to convert tree to dictionary '''\n",
    "      if not tree:\n",
    "          tree = self.root\n",
    "\n",
    "      if tree.output is not None:\n",
    "          return tree.output\n",
    "\n",
    "      else:\n",
    "          left = self._tree_to_dict(tree.left)\n",
    "          right = self._tree_to_dict(tree.right)\n",
    "          \n",
    "      return {\"feature\": \"X_\" + str(tree.feature),\n",
    "                \"threshold\": tree.threshold,\n",
    "                \"info_gain\": round(tree.info_gain, 4),\n",
    "                \"left\": left,\n",
    "                \"right\": right}\n",
    "\n",
    "  '''\n",
    "  Defines the \"visualize_tree\" function which returns tree diagram of trained decision tree\n",
    "  '''\n",
    "\n",
    "  def visualize_tree(self):\n",
    "    graph = pydot.Dot(graph_type='digraph')\n",
    "    tree_dict = self.tree\n",
    "    split_index = [0]  # List to keep track of split index\n",
    "\n",
    "    def add_node(node_dict):\n",
    "        node_label = f\"{node_dict['feature']} <= {node_dict['threshold']}\"\n",
    "        node = pydot.Node(node_label)\n",
    "        graph.add_node(node)\n",
    "        \n",
    "        if isinstance(node_dict['left'], dict):\n",
    "            left_node = add_node(node_dict['left'])\n",
    "            graph.add_edge(pydot.Edge(node, left_node, label=\"True\"))\n",
    "        else:\n",
    "            lbl_l = str(node_dict['left'])\n",
    "            leaf_node_label = f\"Leaf-{split_index[0]} \\n Class-{lbl_l} \\n\"\n",
    "            split_index[0] += 1  # Increment split index\n",
    "            leaf_node = pydot.Node(leaf_node_label, shape='box', style=\"filled\", fillcolor=\"lightgreen\")\n",
    "            graph.add_node(leaf_node)\n",
    "            graph.add_edge(pydot.Edge(node, leaf_node, label=\"True\"))\n",
    "        \n",
    "        if isinstance(node_dict['right'], dict):\n",
    "            right_node = add_node(node_dict['right'])\n",
    "            graph.add_edge(pydot.Edge(node, right_node, label=\"False\"))\n",
    "        else:\n",
    "            lbl_r = str(node_dict['right'])\n",
    "            leaf_node_label = f\"Leaf-{split_index[0]} \\n Class-{lbl_r} \\n\"\n",
    "            split_index[0] += 1  # Increment split index\n",
    "            leaf_node = pydot.Node(leaf_node_label, shape='box', style=\"filled\", fillcolor=\"lightblue\")\n",
    "            graph.add_node(leaf_node)\n",
    "            graph.add_edge(pydot.Edge(node, leaf_node, label=\"False\"))\n",
    "        \n",
    "        return node\n",
    "\n",
    "    add_node(tree_dict)\n",
    "    # Save the image to a file\n",
    "    with open('clf.png', 'wb') as f:\n",
    "        f.write(graph.create_png())\n",
    "    Image.open(BytesIO(graph.create_png())).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f11f64f2",
   "metadata": {},
   "source": [
    "# Testing on Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "30e2f8b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Breast Cancer Dataset\n",
      "our DT:\n",
      "Build time 0.7762 seconds\n",
      "Accuracy : 0.965034965034965\n",
      "SKLearn DT:\n",
      "Build time 0.7823 seconds\n",
      "Accuracy : 0.9440559440559441\n",
      "*************************************\n",
      "Iris Dataset\n",
      "our DT:\n",
      "Build time 0.0321 seconds\n",
      "Accuracy : 0.9736842105263158\n",
      "SKLearn DT:\n",
      "Build time 0.0333 seconds\n",
      "Accuracy : 1.0\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "\n",
    "\n",
    "### validation 1- breast cancer dataset\n",
    "\n",
    "data = datasets.load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42\n",
    ")\n",
    "\n",
    "# Train the decision tree with entropy method\n",
    "tic = time.perf_counter()\n",
    "clf = DecisionTreeClassifier(min_samples_split= 30, max_depth= 8, n_features = 5, criterion = 'entropy')\n",
    "clf.train(X_train, y_train)\n",
    "toc = time.perf_counter()\n",
    "print(\"Breast Cancer Dataset\")\n",
    "print(\"our DT:\")\n",
    "print(f\"Build time {toc - tic:0.4f} seconds\")\n",
    "\n",
    "predictions = clf.predict(X_test)\n",
    "\n",
    "def accuracy(y_test, y_pred):\n",
    "    return np.sum(y_test == y_pred) / len(y_test)\n",
    "\n",
    "acc = accuracy(y_test, predictions)\n",
    "print(\"Accuracy : \" + str(acc))\n",
    "\n",
    "#inbuilt dt\n",
    "clf_i = tree.DecisionTreeClassifier()\n",
    "clf_i.fit(X_train, y_train)\n",
    "toc = time.perf_counter()\n",
    "print(\"SKLearn DT:\")\n",
    "print(f\"Build time {toc - tic:0.4f} seconds\")\n",
    "\n",
    "predictions = clf_i.predict(X_test)\n",
    "acc = accuracy(y_test, predictions)\n",
    "print(\"Accuracy : \" + str(acc))\n",
    "print(\"*************************************\")\n",
    "\n",
    "### validation 2- iris dataset\n",
    "data = datasets.load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42\n",
    ")\n",
    "\n",
    "# Train the decision tree with entropy method\n",
    "tic = time.perf_counter()\n",
    "clf = DecisionTreeClassifier(min_samples_split= 30, max_depth= 8, n_features = 5, criterion = 'entropy')\n",
    "clf.train(X_train, y_train)\n",
    "toc = time.perf_counter()\n",
    "print(\"Iris Dataset\")\n",
    "print(\"our DT:\")\n",
    "print(f\"Build time {toc - tic:0.4f} seconds\")\n",
    "\n",
    "predictions = clf.predict(X_test)\n",
    "\n",
    "def accuracy(y_test, y_pred):\n",
    "    return np.sum(y_test == y_pred) / len(y_test)\n",
    "\n",
    "acc = accuracy(y_test, predictions)\n",
    "print(\"Accuracy : \" + str(acc))\n",
    "\n",
    "#inbuilt dt\n",
    "clf_i = tree.DecisionTreeClassifier()\n",
    "clf_i.fit(X_train, y_train)\n",
    "toc = time.perf_counter()\n",
    "print(\"SKLearn DT:\")\n",
    "print(f\"Build time {toc - tic:0.4f} seconds\")\n",
    "\n",
    "predictions = clf_i.predict(X_test)\n",
    "acc = accuracy(y_test, predictions)\n",
    "print(\"Accuracy : \" + str(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "176e332b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_2 <= 1.9 ? 0.896\n",
      " left:0\n",
      " right:X_2 <= 4.7 ? 0.6276\n",
      "  left:X_3 <= 1.6 ? 0.1872\n",
      "    left:1\n",
      "    right:2\n",
      "  right:X_2 <= 5.1 ? 0.1853\n",
      "    left:2\n",
      "    right:2\n"
     ]
    }
   ],
   "source": [
    "clf.print_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "17824c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.visualize_tree()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
