{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffa4cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ee5a5f",
   "metadata": {},
   "source": [
    "## Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c01224f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node_reg:\n",
    "    \"\"\"\n",
    "    Initializes \"Node class constructor\" for regression problems which stores and initialises:\n",
    "    Feature index used for best split :  feature = None,\n",
    "    Left child node :  left_spl = None,\n",
    "    Right child node :  right_spl = None,\n",
    "    Node Splitting threshold value :  spl_val = None,\n",
    "    Variance loss value:  var_redn = None,\n",
    "    Output value of the node :  out_val = None\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        feature=None,\n",
    "        spl_val=None,\n",
    "        left_spl=None,\n",
    "        right_spl=None,\n",
    "        var_redn=None,\n",
    "        *,\n",
    "        out_val=None\n",
    "    ):\n",
    "        self.feature = feature\n",
    "        self.threshold = spl_val\n",
    "        self.left = left_spl\n",
    "        self.right = right_spl\n",
    "        self.var_redn = var_redn\n",
    "        self.output = out_val\n",
    "\n",
    "    # function to check whether a leaf node or not\n",
    "    def is_leaf_node(self):\n",
    "        return self.output is not None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00f0c559",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeRegressor:\n",
    "    \"\"\"\n",
    "    Initializes \"Decision Tree constructor\" for regression problems which stores and initialises:\n",
    "    Maximum tree depth : max_depth=100,\n",
    "    Minimum samples reuqired for splitting nodes : min_samples_split=2,\n",
    "    Number of features to be used for tree: n_features = None,\n",
    "    Cost function/ Loss criterion for splitting nodes : criterion='mse'\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, max_depth=100, min_samples_split=2, n_features=None, criterion=\"mse\"\n",
    "    ):\n",
    "\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.n_features = n_features\n",
    "        self.criterion = criterion\n",
    "        self.root = None\n",
    "\n",
    "    # # For storing tree structure to generate tree graph visualization\n",
    "    #     self.tree = None\n",
    "    #     self.graph = None\n",
    "    #     self.list_node_names = []\n",
    "\n",
    "    \"\"\"\n",
    "  Defines the train method of the DecisionTree class, \n",
    "  which takes two inputs: \n",
    "  1. X, a matrix of input features\n",
    "  2. y, a vector of output labels\n",
    "  Then, uses the _build_tree function to proceed with growing the decision tree structure\n",
    "  \"\"\"\n",
    "\n",
    "    def train(self, X, y):\n",
    "        # sets self.n_features to number of features in the input if n_features is not specified,\n",
    "        # Else, the minimum of n_features and the number of features in the input data if n_features is specified to avoid exceeding the maximum feature limit\n",
    "\n",
    "        self.n_features = (\n",
    "            X.shape[1] if not self.n_features else min(self.n_features, X.shape[1])\n",
    "        )\n",
    "        self.root = self._build_tree(X, y)\n",
    "\n",
    "    \"\"\"\n",
    "  Defines the recursive \"_build_tree\" function of the DecisionTree class, \n",
    "  which takes two inputs: \n",
    "  1. X, a matrix of input features\n",
    "  2. y, a vector of output labels\n",
    "\n",
    "  Returns Tree Nodes after each iteration\n",
    "  \"\"\"\n",
    "\n",
    "    def _build_tree(self, X, y, depth=0):\n",
    "        # stores the number of observations and features in the input data\n",
    "        n_obs, n_fts = X.shape\n",
    "\n",
    "        # checks for stopping conditions of a decision tree, if met returns the leaf node\n",
    "        if depth >= self.max_depth or n_obs <= self.min_samples_split:\n",
    "            leaf_val = self._calculate_node_mean(y)\n",
    "            return Node_reg(out_val=leaf_val)\n",
    "\n",
    "        # checks for all possible splits across features and returns the best split threshold and feature column index\n",
    "        best_var_redn = -float(\"inf\")\n",
    "        best_feature, best_thresh = None, None\n",
    "        criterion = self.criterion\n",
    "\n",
    "        # random selection of specified number of features out of all the available features\n",
    "        select_fts = np.random.choice(n_fts, self.n_features, replace=False)\n",
    "\n",
    "        for feature in select_fts:\n",
    "            thresholds = np.unique(X[:, feature])\n",
    "            for thresh in thresholds:\n",
    "                var_redn = self._variance_reduction(y, X[:, feature], thresh, criterion)\n",
    "                if var_redn > best_var_redn:\n",
    "                    best_var_redn = var_redn\n",
    "                    best_feature = feature\n",
    "                    best_thresh = thresh\n",
    "\n",
    "        # create child nodes\n",
    "        left_idxs, right_idxs = self._split(X[:, best_feature], best_thresh)\n",
    "        left = self._build_tree(X[left_idxs, :], y[left_idxs], depth + 1)\n",
    "        right = self._build_tree(X[right_idxs, :], y[right_idxs], depth + 1)\n",
    "\n",
    "        return Node_reg(\n",
    "            feature=best_feature,\n",
    "            spl_val=best_thresh,\n",
    "            left_spl=left,\n",
    "            right_spl=right,\n",
    "            var_redn=best_var_redn,\n",
    "        )\n",
    "\n",
    "    \"\"\"\n",
    "  Defines the function to compute leaf node value \n",
    "  which takes input as y, a vector of output labels and returns the mean\n",
    "  \"\"\"\n",
    "\n",
    "    def _calculate_node_mean(self, y):\n",
    "        \"\"\" \"\"\"\n",
    "        val = np.mean(y)\n",
    "        return val\n",
    "\n",
    "    \"\"\"\n",
    "  Defines the node splitting function which takes as input\n",
    "  data : the subset input data based on a feature index\n",
    "  split_thresh : Specified split threshold to split the tree in left and right child nodes\n",
    "\n",
    "  Returns : Indices of left and right child nodes\n",
    "  \"\"\"\n",
    "\n",
    "    def _split(self, data, split_thresh):\n",
    "        left_idxs = np.argwhere(data <= split_thresh).flatten()\n",
    "        right_idxs = np.argwhere(data > split_thresh).flatten()\n",
    "        return left_idxs, right_idxs\n",
    "\n",
    "    \"\"\"\n",
    "  Defines the information_gain funtion which takes as input\n",
    "  y_parent : Parent node data for labels\n",
    "  X_ft : the subset input data based on a feature index\n",
    "  split_thresh : Specified split threshold to split the tree in left and right child nodes\n",
    "  criterion : Criterion to calculate node impurity values, mean-squared-error or mean-absolute-error for regression tasks\n",
    "\n",
    "  Returns : Information gain value, which is variance reduction for regression, as we move from a parent node to child nodes\n",
    "  \"\"\"\n",
    "\n",
    "    def _variance_reduction(self, y_parent, X_ft, threshold, criterion):\n",
    "        if criterion.lower() == \"mse\":\n",
    "            # parent entropy\n",
    "            parent_mse = self._mse_loss(y_parent)\n",
    "\n",
    "            # create children\n",
    "            left_idxs, right_idxs = self._split(X_ft, threshold)\n",
    "            if len(left_idxs) == 0 or len(right_idxs) == 0:\n",
    "                return 0\n",
    "\n",
    "            # calculate the weighted avg. entropy of children\n",
    "            n = len(y_parent)\n",
    "            p_left, p_right = len(left_idxs) / n, len(right_idxs) / n\n",
    "            e_left, e_right = self._mse_loss(y_parent[left_idxs]), self._mse_loss(\n",
    "                y_parent[right_idxs]\n",
    "            )\n",
    "\n",
    "            child_mse = p_left * e_left + p_right * e_right\n",
    "\n",
    "            # calculate the IG\n",
    "            information_gain = parent_mse - child_mse\n",
    "\n",
    "        elif criterion.lower() == \"mae\":\n",
    "            # parent gini impurity\n",
    "            parent_mae = self._mae_loss(y_parent)\n",
    "\n",
    "            # create children\n",
    "            left_idxs, right_idxs = self._split(X_ft, threshold)\n",
    "            if len(left_idxs) == 0 or len(right_idxs) == 0:\n",
    "                return 0\n",
    "\n",
    "            # calculate the weighted avg. gini impurity of children\n",
    "            n = len(y_parent)\n",
    "            p_left, p_right = len(left_idxs) / n, len(right_idxs) / n\n",
    "            g_left, g_right = self._mae_loss(y_parent[left_idxs]), self._mae_loss(\n",
    "                y_parent[right_idxs]\n",
    "            )\n",
    "\n",
    "            child_mae = p_left * g_left + p_right * g_right\n",
    "\n",
    "            # calculate the IG\n",
    "            information_gain = parent_mae - child_mae\n",
    "\n",
    "        else:\n",
    "            information_gain = -1\n",
    "\n",
    "        return information_gain\n",
    "\n",
    "    \"\"\"\n",
    "  Defines the MSE_loss function which takes as input\n",
    "  y_parent : data of label for a node\n",
    "\n",
    "  Returns : Mean squared error of the node\n",
    "  \"\"\"\n",
    "\n",
    "    def _mse_loss(self, y):\n",
    "        n = len(y)\n",
    "        m = np.mean(y)\n",
    "        return np.sum(np.square(y - m)) / n\n",
    "\n",
    "    \"\"\"\n",
    "  Defines the MAE_loss function which takes as input\n",
    "  y_parent : data of label for a node\n",
    "\n",
    "  Returns : Mean absolute error of the node\n",
    "  \"\"\"\n",
    "\n",
    "    def _mae_loss(self, y):\n",
    "        n = len(y)\n",
    "        m = np.mean(y)\n",
    "        return np.sum(np.abs(y - m)) / n\n",
    "\n",
    "    \"\"\"\n",
    "  Defines the predict method of the Decision Tree Regressor which takes as input\n",
    "  X : Test/ Valdiation dataset for features\n",
    "\n",
    "  Returns : Array of Prediction classes after traversing through each observation or row in \"X\"\n",
    "  \"\"\"\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._traverse_dtree(x, self.root) for x in X])\n",
    "\n",
    "    \"\"\"\n",
    "  Defines the recursive \"_traverse_dtree\" function which takes as input\n",
    "  x : one single row/ observation of the test/validation data\n",
    "  node : information of parent node to traverse down the tree to leaf\n",
    "  \"\"\"\n",
    "\n",
    "    def _traverse_dtree(self, x, node):\n",
    "\n",
    "        # Checks for leaf node, if satisfied returns the node output value, else keeps on traversing\n",
    "        if node.is_leaf_node():\n",
    "            return node.output\n",
    "\n",
    "        # Checks if observation has the feature with value according to left child node (<= threshold) and keeps on traversing in the left splits\n",
    "        if x[node.feature] <= node.threshold:\n",
    "            return self._traverse_dtree(x, node.left)\n",
    "\n",
    "        # Finally traverses on the right child node\n",
    "        return self._traverse_dtree(x, node.right)\n",
    "\n",
    "    \"\"\"\n",
    "  Defines the recursive \"_traverse_dtree\" function which takes as input\n",
    "  x : one single row/ observation of the test/validation data\n",
    "  node : information of parent node to traverse down the tree to leaf\n",
    "  \"\"\"\n",
    "\n",
    "    def print_tree(self, tree=None, indent=\" \"):\n",
    "        \"\"\"function to print the tree\"\"\"\n",
    "        if not tree:\n",
    "            tree = self.root\n",
    "\n",
    "        if tree.output is not None:\n",
    "            print(tree.output)\n",
    "\n",
    "        else:\n",
    "            print(\n",
    "                \"X_\" + str(tree.feature),\n",
    "                \"<=\",\n",
    "                tree.threshold,\n",
    "                \"?\",\n",
    "                round(tree.var_redn, 4),\n",
    "            )\n",
    "            print(\"%sleft:\" % (indent), end=\"\")\n",
    "            self.print_tree(tree.left, indent + indent)\n",
    "            print(\"%sright:\" % (indent), end=\"\")\n",
    "            self.print_tree(tree.right, indent + indent)\n",
    "\n",
    "            # # 3. Record the subtree\n",
    "            # condition = \"X_\"+ str(tree.feature) + \"<=\" + str(tree.threshold)\n",
    "            # result_tree = {condition: []}\n",
    "\n",
    "            # result_tree[condition].append(self.print_tree(tree.left))\n",
    "            # result_tree[condition].append(self.print_tree(tree.right))\n",
    "\n",
    "    # def to_dot(self):\n",
    "    #       dot_data = export_graphviz(self.train, out_file=None,\n",
    "    #                                  feature_names=self.feature_names,\n",
    "    #                                  class_names=self.class_names,\n",
    "    #                                  filled=True, rounded=True,\n",
    "    #                                  special_characters=True)\n",
    "    #       return dot_data\n",
    "\n",
    "    # def plot_tree(self):\n",
    "    #       dot_data = self.to_dot()\n",
    "    #       graph = graphviz.Source(dot_data)\n",
    "    #       display(graph)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
